{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Description: Datasets modification framework using cluster analysis\n",
    "\n",
    "Author: Jiří Setinský\n",
    "\n",
    "Date: 2.5. 2023\n",
    "\n",
    "Contact: xsetin00@stud.fit.vutbr.cz"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "wPe_TlSVBXGI"
   },
   "source": [
    "\n",
    "\n",
    "# Dependencies\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QuIEGjwGo5GJ",
    "outputId": "b7d576a3-23b4-4ca7-a153-1af65b4bf27f"
   },
   "outputs": [],
   "source": [
    "\n",
    "#!pip install h2o \n",
    "#!pip install joblib -U\n",
    "\n",
    "#!pip install yellowbrick\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "import h2o\n",
    "from h2o.estimators import H2OKMeansEstimator\n",
    "from sklearn.neighbors import KNeighborsClassifier,NearestNeighbors\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.spatial import cKDTree\n",
    "plt.style.use('default')\n",
    "import math\n",
    "import itertools\n",
    "from abc import ABC, abstractmethod\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.cluster import KMeans,MeanShift,AgglomerativeClustering, MiniBatchKMeans\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.metrics import davies_bouldin_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from joblib import dump, load\n",
    "from joblib import Parallel, delayed\n",
    "from imblearn.under_sampling import RandomUnderSampler,CondensedNearestNeighbour, NearMiss, EditedNearestNeighbours,RepeatedEditedNearestNeighbours,InstanceHardnessThreshold,NeighbourhoodCleaningRule,TomekLinks,ClusterCentroids,OneSidedSelection,AllKNN\n",
    "from yellowbrick.cluster import KElbowVisualizer\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('default')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/netmon/xsetin00/jdk-17/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['JAVA_HOME'] = '/home/netmon/xsetin00/jdk-17'\n",
    "os.environ['PATH'] = '/home/netmon/xsetin00/jdk-17/bin:' + os.environ['PATH']\n",
    "\n",
    "print(os.environ['PATH'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "Njq92IAZJ67G"
   },
   "source": [
    "# Class definitions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "qoG-YsCW7wCp"
   },
   "source": [
    "## Dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "id": "yvqEw_Dr7vRx"
   },
   "outputs": [],
   "source": [
    "class Dataset():\n",
    "  \n",
    "  def __init__(self, path, label, use_case, class_values,to_drop=[]):\n",
    "    self.path = path\n",
    "    self.label = label\n",
    "    self.use_case = use_case\n",
    "    self.pos_value=class_values[0]\n",
    "    self.neg_value=class_values[1]\n",
    "    to_drop.append(label)\n",
    "    self.to_drop=to_drop\n",
    "    self.total_acc={}\n",
    "  \n",
    "  # Rename class values\n",
    "  def new_classes(self,label):\n",
    "    if label == self.pos_value:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "  # Load data\n",
    "  def load(self):\n",
    "    self.csv_data=pd.read_csv(self.path)\n",
    "    label_encoder = LabelEncoder()\n",
    "    self.csv_data[self.label] = label_encoder.fit_transform(self.csv_data[self.label])\n",
    "    scaler=StandardScaler()\n",
    "    features=self.csv_data.drop(self.label, axis=1).columns\n",
    "    self.csv_data[features]=scaler.fit_transform(self.csv_data[features])\n",
    "\n",
    "    return self.csv_data\n",
    "  \n",
    "  # Set internal data\n",
    "  def set_data(self,data):\n",
    "    self.csv_data=data\n",
    "  \n",
    "  # Get label data for current class\n",
    "  def get_label(self,data=None):\n",
    "    if data is None:\n",
    "      return self.csv_data[self.label][self.csv_data[self.label]==self.flag].reset_index(drop=True)\n",
    "    return data[self.label][data[self.label]==self.flag].reset_index(drop=True)\n",
    "\n",
    "  # Get features data for current class\n",
    "  def get_features(self,data=None):\n",
    "    if data is None:\n",
    "      return self.csv_data[self.csv_data[self.label]==self.flag].drop(columns=self.to_drop,errors=\"ignore\").reset_index(drop=True)\n",
    "    return data[data[self.label]==self.flag].drop(columns=self.to_drop,errors=\"ignore\").reset_index(drop=True)\n",
    "  \n",
    "  # Get label data for all classes\n",
    "  def get_all_label(self):\n",
    "    return self.csv_data[self.label].reset_index(drop=True)\n",
    "\n",
    "  # Get features data for all classes\n",
    "  def get_all_features(self):\n",
    "    return self.csv_data.drop(columns=self.to_drop,errors=\"ignore\").reset_index(drop=True)\n",
    "\n",
    "  # Set context to current class\n",
    "  def set_class(self,flag):\n",
    "    self.flag=flag\n",
    "    self.y=self.get_label()\n",
    "    self.X=self.get_features()\n",
    "  \n",
    "  # Get class balance\n",
    "  def class_balance(self,data=None):\n",
    "    if data is None:\n",
    "      data=self.get_all_label()\n",
    "    pos=data[data==1].shape[0]\n",
    "    neg=data.shape[0]-pos\n",
    "    return {0:neg,1:pos,'all':data.shape[0]}\n",
    "\n",
    "  # Correlation with class\n",
    "  def get_corr_analysis(self):\n",
    "    correlation_with_class=self.csv_data.drop(columns=[self.label]).corrwith(self.csv_data[self.label],method='spearman').reset_index()\n",
    "    #print(correlation_with_class)\n",
    "    correlation_with_class.columns=[\"atrr\",\"corr\"]\n",
    "    \n",
    "    corr_pos=correlation_with_class[correlation_with_class[\"corr\"] >= 0.3]\n",
    "    corr_neg=correlation_with_class[correlation_with_class[\"corr\"] <= -0.3]\n",
    "\n",
    "    self.class_corr=pd.concat([corr_pos,corr_neg],axis=0).reset_index(drop=True)\n",
    "    #class_corr.to_csv('/content/drive/My Drive/Cluster/correlation_dga.txt',header=False)\n",
    "    #self.class_corr=pd.read_csv('xsetin00/correlation_dga.txt',header=None,names=[\"atrr\",\"corr\"])##\n",
    "    return self.class_corr\n",
    "    \n",
    "  # Get correlation pairs with opposite coefficient\n",
    "  def get_corr_pairs(self):\n",
    "    #self.class_corr=pd.read_csv('/content/drive/My Drive/Cluster/correlation_dga.txt',header=None,names=[\"atrr\",\"corr\"])\n",
    "    pos_corr=list(self.class_corr[self.class_corr[\"corr\"]>0][\"atrr\"])\n",
    "    neg_corr=list(self.class_corr[self.class_corr[\"corr\"]<0][\"atrr\"])\n",
    "\n",
    "    self.corr_pairs=list(itertools.product(pos_corr,neg_corr))+list(itertools.product(neg_corr,pos_corr))\n",
    "    return self.corr_pairs\n",
    "  \n",
    "  # Classify only one class with given model\n",
    "  def dataset_acc(self,clf,label):\n",
    "    self.set_class(label)\n",
    "    pred=clf.predict(self.X)\n",
    "    ######print(classification_report(self.data.y,pred))#Data[\"is_doh\"], pred))\n",
    "    self.total_acc[label]=metrics.accuracy_score(self.y,pred)\n",
    "    ######metrics.ConfusionMatrixDisplay(metrics.confusion_matrix(self.data.y, pred) ).plot();\n",
    "    #return self.total_acc\n",
    "    return f\"Accuracy for class {label} is {self.total_acc[label]}\"\n",
    "    \n",
    "  # Save data to csv\n",
    "  def save(self,path,header=True,sep=','):\n",
    "      self.csv_data.to_csv(path,header=header,sep=sep,index=False)  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modifier class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Modifier(Dataset):\n",
    "\n",
    "    def __init__(self,new_path,label,use_case,class_values,to_drop=[]):\n",
    "        super().__init__(new_path,label,use_case,class_values,to_drop)\n",
    "        self.newdata=pd.DataFrame()\n",
    "        self.improvement=0\n",
    "\n",
    "    # Update csv data with new data \n",
    "    def update_csv(self):\n",
    "        self.csv_data=self.newdata\n",
    "        self.newdata=pd.DataFrame()\n",
    "\n",
    "    # Set new data  \n",
    "    def set_newdata(self,X=pd.DataFrame(),y=pd.DataFrame()):\n",
    "        self.newdata=pd.concat([X,y],axis=1)\n",
    "\n",
    "    # Save new data to csv\n",
    "    def save_new(self,path,append=False):\n",
    "      self.newdata.to_csv(path,index=False,mode='a' if append else 'w',header=(not append))\n",
    "\n",
    "    def drop_duplicates(self,X,amount):\n",
    "        if X.shape[0] == 1: return X\n",
    "        nbrs = NearestNeighbors(n_neighbors=2, algorithm='ball_tree').fit(X)\n",
    "        distances, indices = nbrs.kneighbors(X)\n",
    "        neighbor_pairs = []\n",
    "        for i in range(len(X)):\n",
    "            distance = distances[i][1]  # \n",
    "            neighbor_index = indices[i][1]  #\n",
    "            neighbor_pairs.append((i, neighbor_index, distance))\n",
    "        neighbor_pairs.sort(key=lambda x: x[2])\n",
    "        #print(\"original size\",len(neighbor_pairs))\n",
    "        points_to_drop = []\n",
    "        for row in neighbor_pairs:\n",
    "            _, neighbour, _ = row\n",
    "            if neighbour not in points_to_drop:\n",
    "                points_to_drop.append(neighbour)\n",
    "        points_to_drop=points_to_drop[:(X.shape[0]-amount)]         \n",
    "        #print(\"to drop\",len(points_to_drop))\n",
    "\n",
    "        return pd.DataFrame(np.delete(X, points_to_drop, axis=0),columns=X.columns)\n",
    "\n",
    "    def sample(self,cluster_id,frac,model,style=\"random\"):\n",
    "        data_to_sample=model.result_new[model.result_new[\"predict\"]==cluster_id]\n",
    "        print(\"data to sample\",data_to_sample.shape)\n",
    "        if style == \"random\":\n",
    "            sampled_data=data_to_sample.sample(frac=frac,random_state= np.random.RandomState())#ranmagic,random_state=42)\n",
    "        elif style == \"1nn\":\n",
    "            amount=int(round(frac*data_to_sample.shape[0],0))\n",
    "            print(\"amount\",amount,\"frac\",frac)\n",
    "            if amount == 0: return pd.DataFrame()\n",
    "            #if amount <= 1: amount=2\n",
    "            sampled_data=self.drop_duplicates(data_to_sample,amount)\n",
    "            while sampled_data.shape[0] != amount: \n",
    "                sampled_data=self.drop_duplicates(sampled_data,amount)\n",
    "            print(\"1nn extracted\",sampled_data.shape[0])\n",
    "        elif style == \"clustercentroids\":\n",
    "            k=int(round(frac*data_to_sample.shape[0],0))\n",
    "            if k == 0: return pd.DataFrame()\n",
    "            kmeans=KMeans(n_clusters=k,max_iter=100,n_init=\"auto\")\n",
    "            kmeans.fit(data_to_sample)\n",
    "            sampled_data=kmeans.cluster_centers_\n",
    "            sampled_data=pd.DataFrame(sampled_data,columns=data_to_sample.columns)\n",
    "            print(\"centroids extracted\",sampled_data.shape)\n",
    "        else:\n",
    "            print(model.coeffs,model.centers)\n",
    "            centroid=model.centers\n",
    "            print(centroid)\n",
    "            cluster_data=data_to_sample.join(self.X)\n",
    "            print(cluster_data)\n",
    "            cluster_data.pop(\"predict\")\n",
    "            prep_data=model.prep.transform(cluster_data)\n",
    "            print(prep_data)\n",
    "            distances = list(map(lambda x: np.linalg.norm(x - centroid),prep_data))\n",
    "            print(distances)\n",
    "            print(math.ceil(frac*data_to_sample.shape[0]))\n",
    "            sampled_data = np.argsort(distances)[:math.ceil(frac*data_to_sample.shape[0])]\n",
    "            sampled_data =pd.DataFrame(sampled_data,columns=[\"predict\"],index=sampled_data)\n",
    "            \n",
    "        #print(sampled_data)\n",
    "        return sampled_data\n",
    "    \n",
    "    # Extract demanded data from each cluster based on weight vector\n",
    "    def extract(self,cluster_model,reduced_amount,oversample=True):\n",
    "        self.set_class(cluster_model.class_number)\n",
    "        cluster_model.data.set_class(cluster_model.class_number)\n",
    "        cluster_model.predict(self)\n",
    "        self.get_data_amount(cluster_model,reduced_amount,oversample)\n",
    "        print(\"POTENTIAL AMOUNT\",self.potential_amount)\n",
    "        print(\"REDUCED AMOUNT\",reduced_amount)\n",
    "        aug_weight=min(reduced_amount/self.potential_amount,1)\n",
    "        new_data_sampled=pd.DataFrame()\n",
    "        for cluster_id,coeff in cluster_model.coeffs.items():\n",
    "           \n",
    "            final_frac=min(coeff*self.weight,1)*aug_weight\n",
    "            data_to_sample=self.sample(cluster_id,final_frac,cluster_model,style=\"clustercentroids\")\n",
    "            \n",
    "            new_data_sampled=pd.concat([new_data_sampled,data_to_sample])\n",
    "\n",
    "        new_data_sampled=new_data_sampled.join(self.X)\n",
    "        new_data_sampled=new_data_sampled.join(self.y)\n",
    "        new_data_sampled.pop(\"predict\")\n",
    "        #print(new_data_sampled)\n",
    "        print(\"Extracted data\",cluster_model.class_number,\" - \",new_data_sampled.shape[0])\n",
    "        #self.newdata=pd.concat([self.newdata,new_data_sampled])\n",
    "        return new_data_sampled\n",
    "\n",
    "\n",
    "    # Find potential amount of data to extract \n",
    "    def get_data_amount(self,model,total_demand,oversample):\n",
    "        self.potential_amount=0\n",
    "        clusters,counts=np.unique(model.result_new,return_counts=True)\n",
    "\n",
    "        for cluster,amount in zip(clusters,counts):\n",
    "            self.potential_amount+=model.coeffs[cluster]*amount\n",
    "\n",
    "        self.potential_amount=int(self.potential_amount)\n",
    "        reduce_ratio=total_demand/self.potential_amount\n",
    "        if reduce_ratio > 1 and oversample:\n",
    "            self.weight=reduce_ratio*1.11\n",
    "            self.potential_amount=int(sum([(min(model.coeffs[cluster]*self.weight,1))*amount for cluster,amount in zip(clusters,counts)]))\n",
    "        else:\n",
    "            self.weight=1\n",
    "\n",
    "    # Plot class balance\n",
    "    def plot_balance(self,title,data=None):\n",
    "        if data is None:\n",
    "            data=self.get_all_label()\n",
    "        ax=data.value_counts().plot.pie(title=f\"{title}\\n\\n\"+\"{:,}\".format(data.shape[0]),autopct='%1.1f%%')\n",
    "        fig=ax.get_figure()\n",
    "        #fig.savefig(f\"xsetin00/{self.use_case}/{title}.png\",dpi=500)\n",
    "        plt.show()\n",
    "\n",
    "    # Plot classification report\n",
    "    def plot_report(self,report,title):\n",
    "\n",
    "        ax=sns.heatmap(report.iloc[:-1, :].T, annot=True,fmt='.4f')\n",
    "        #total_acc=metrics.accuracy_score(y,pred)\n",
    "        plt.yticks(rotation=0)\n",
    "        plt.title(title) \n",
    "        #report.transpose().to_latex(f\"xsetin00/{self.use_case}/{title} report.tex\",float_format=\"{:.4f}\".format,caption=title,label=f\"tab:{title}\")\n",
    "        #ax.get_figure().savefig(f\"xsetin00/{self.use_case}/{title} report.png\",dpi=500)\n",
    "        plt.show()\n",
    "\n",
    "    # Compare model on old data between model on clustered data\n",
    "    def compare_models(self,old,new,test_data,compared_model=\"Clustered\"):\n",
    "\n",
    "        X_train=self.newdata.drop(columns=self.label).reset_index(drop=True)\n",
    "        y_train=self.newdata[self.label].reset_index(drop=True)\n",
    "        new.fit(X_train,y_train)\n",
    "\n",
    "        self.cnt_old=self.class_balance(self.get_all_label())\n",
    "        print(\"Old dataset balance\",self.cnt_old)\n",
    "        self.plot_balance(\"Old dataset balance\",self.get_all_label())\n",
    "\n",
    "        self.cnt_new=self.class_balance(y_train)\n",
    "        print(f\"{compared_model} dataset balance\",self.cnt_new)\n",
    "        self.plot_balance(f\"{compared_model} dataset balance\",y_train)\n",
    "\n",
    "\n",
    "        print(\"Reduction ratio\",100-self.cnt_new['all']/self.cnt_old['all']*100)\n",
    "        X=test_data.get_all_features()\n",
    "        y=test_data.get_all_label()\n",
    "\n",
    "        pred_old = old.predict(X)\n",
    "        report_old=pd.DataFrame(classification_report(y,pred_old,digits=4,output_dict=True))\n",
    "        self.plot_report(report_old,\"Old model\")\n",
    "        self.plot_matrix(y,pred_old,\"Old model CM\")\n",
    "\n",
    "\n",
    "        pred_new = new.predict(X)\n",
    "        report_new=pd.DataFrame(classification_report(y,pred_new,digits=4,output_dict=True))\n",
    "        self.plot_report(report_new,f\"{compared_model} model\")\n",
    "        self.plot_matrix(y,pred_old,f\"{compared_model} model CM\")\n",
    "\n",
    "\n",
    "        f1_old=metrics.f1_score(y,pred_old,average=\"weighted\")\n",
    "        f1_new=metrics.f1_score(y,pred_new,average=\"weighted\")\n",
    "        #print(f1_old,f1_new)\n",
    "        space_to_improve=1-f1_old\n",
    "        self.improvement=(f1_new-f1_old)/space_to_improve*100\n",
    "        print(\"Improvement\",self.improvement)\n",
    "\n",
    "        return classification_report(y,pred_new,digits=4,output_dict=True)\n",
    "\n",
    "    # Apply existing methods for data modification \n",
    "    def apply_imbalanced_lib(self,sampler,prep=False):\n",
    "        X_old=self.get_all_features()\n",
    "        y_old=self.get_all_label()\n",
    "        if prep:\n",
    "            pipe=make_pipeline(StandardScaler(),VarianceThreshold())\n",
    "            X_res, y_res = sampler.fit_resample(pipe.fit_transform(X_old), y_old)\n",
    "            X_res=pipe.inverse_transform(X_res)\n",
    "            X_res=pd.DataFrame(X_res, columns = X_old.columns)\n",
    "        else:\n",
    "            X_res, y_res = sampler.fit_resample(X_old, y_old)\n",
    "            \n",
    "        self.set_newdata(X_res, y_res)\n",
    "\n",
    "    # Model results to export\n",
    "    def prepare_model_result(self,report,method,dataset,metric='default'):\n",
    "        #Methods OA Precision Recall F measure\n",
    "        return pd.DataFrame({'Dataset':dataset, 'Method':method,'Metric':metric, 'Acc':report['accuracy'], 'Precision':report['weighted avg']['precision'], 'Recall':report['weighted avg']['recall'], 'F1':report['weighted avg']['f1-score'],\"Impr\":self.improvement,\"Support_0\":str(self.cnt_new[0]),\"Support_1\":str(self.cnt_new[1])},index=[0])\n",
    "\n",
    "    # Plot confusion matrix\n",
    "    def plot_matrix(self,y,pred,title):\n",
    "        matrix=metrics.ConfusionMatrixDisplay( metrics.confusion_matrix(y, pred) )\n",
    "        matrix.plot()\n",
    "        plt.show()\n",
    "        # fig=matrix.figure_\n",
    "        # fig.set_figwidth(8)\n",
    "        # fig.set_figheight(8)  \n",
    "        # fig.savefig(f\"xsetin00/{self.use_case}/{title}.png\",dpi=500)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "Y3zOkaJfndzC"
   },
   "source": [
    "## Cluster class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "id": "zw3nEuN0h1ol"
   },
   "outputs": [],
   "source": [
    "class Cluster():\n",
    "  \n",
    "  def __init__(self):\n",
    "    self.data = []\n",
    "    self.report = {}\n",
    "  \n",
    "  # Append row to cluster\n",
    "  def add_data(self, row):\n",
    "    self.data.append(row)\n",
    "\n",
    "  # Convert data to numpy array\n",
    "  def data_to_array(self,columns):\n",
    "    self.collumns=np.array(columns)\n",
    "    self.array = np.array(self.data)\n",
    "\n",
    "  # Get accuracy of cluster \n",
    "  def classify_cluster(self, classifier, label):\n",
    "    pred = classifier.predict(pd.DataFrame(self.array, columns=self.collumns))\n",
    "    y_gen = np.repeat(label, self.array.shape[0])\n",
    "    self.report[\"acc\"] = metrics.accuracy_score(y_gen, pred)\n",
    "\n",
    "  # Variance of cluster's features\n",
    "  def variance(self):\n",
    "    self.report[\"var\"] = np.var(self.array, axis=0)\n",
    "\n",
    "  # Mean of cluster's features\n",
    "  def get_mean(self):\n",
    "    self.mean = np.mean(self.array, axis=0)\n",
    "    self.report[\"mean\"] = self.mean\n",
    "\n",
    "  # Deviation of cluster's features\n",
    "  def diff(self, original_mean, min_val, max_val):\n",
    "    # average deviation of features\n",
    "    diff = self.mean - original_mean\n",
    "    # get maximal difference\n",
    "    lower_min = np.abs(original_mean - min_val)\n",
    "    lower_max = np.abs(original_mean - max_val)\n",
    "\n",
    "    diff_result = np.zeros_like(diff)\n",
    "\n",
    "    for i, value in enumerate(diff):\n",
    "        if value == 0:\n",
    "            diff_result[i] = 0\n",
    "        else:\n",
    "            if value < 0:\n",
    "                diff_result[i] = value / lower_min[i]\n",
    "            else:\n",
    "                diff_result[i] = value / lower_max[i]\n",
    "\n",
    "    self.report[\"dev\"] = diff_result\n",
    "    #print(\"DEVIATION\",self.report['dev'])\n",
    "\n",
    "  # Correlation of cluster's features\n",
    "  def correlation(self):\n",
    "    pairs_to_drop = set()\n",
    "    dataframe=pd.DataFrame(self.array,columns=self.collumns)\n",
    "    cols = dataframe.columns\n",
    "    for i in range(0, dataframe.shape[1]):\n",
    "      for j in range(0, i+1):\n",
    "        pairs_to_drop.add((cols[i], cols[j]))\n",
    "    out=dataframe.corr(method='spearman').unstack().drop(labels=pairs_to_drop).sort_values().dropna()\n",
    "    ######print(out.to_string())\n",
    "    self.report[\"corr\"]=out\n",
    "    #self.report[\"corr\"] = np.corrcoef(self.array.)\n",
    "    #print(\"CORRELATION\",self.report['corr'])\n",
    "\n",
    "  # Size of cluster\n",
    "  def size(self):\n",
    "    self.report[\"size\"] = self.array.shape[0]\n",
    "\n",
    "  # Relative size of cluster\n",
    "  def inv_amount(self, original_size):\n",
    "    self.amount_ratio = self.report[\"size\"] / original_size\n",
    "    self.inv_amount_ratio = 1 - self.amount_ratio\n",
    "\n",
    "  # Average deviation of cluster's features\n",
    "  def diff_deviation(self):\n",
    "    self.extreme_deviation = self.report[\"dev\"][np.logical_or(self.report[\"dev\"] < -50, self.report[\"dev\"] > 50)]\n",
    "    self.mean_deviation = np.mean(np.abs(self.report[\"dev\"]))\n",
    "\n",
    "    extreme_deviation_cnt = len(self.extreme_deviation)\n",
    "    self.ratio_extreme_deviation = extreme_deviation_cnt / len(self.report[\"dev\"])\n",
    "\n",
    "  # Adversial distance of cluster's features\n",
    "  def adversial(self, data):\n",
    "    #corr_pos = self.report[\"corr\"][self.report[\"corr\"] >= 0.75]\n",
    "    corr_neg = self.report[\"corr\"][self.report[\"corr\"] <= -0.3]\n",
    "    self.adversial_distance = 0\n",
    "    corr_cnt = 0\n",
    "\n",
    "    if corr_neg.shape[0] != 0:\n",
    "        for i in range(corr_neg.shape[0]):\n",
    "            corr = corr_neg[i]\n",
    "            if corr in data.corr_pairs:\n",
    "                corr_cnt += 1\n",
    "                first = corr[0]\n",
    "                second = corr[1]\n",
    "                self.adversial_distance += np.abs(self.report[\"dev\"][np.where(self.collumns==first)] - self.report[\"dev\"][np.where(self.collumns==second)])\n",
    "        if corr_cnt != 0:\n",
    "            self.adversial_distance /= corr_cnt * 2\n",
    "            self.adversial_distance = 1 - self.adversial_distance\n",
    "  \n",
    "  # Similarity of cluster's features to another class           \n",
    "  def similarity(self,data):\n",
    "    neg_attr=list(data.class_corr[data.class_corr[\"corr\"]<0][\"atrr\"])\n",
    "    pos_attr=list(data.class_corr[data.class_corr[\"corr\"]>0][\"atrr\"])\n",
    "    #print(data.class_corr)\n",
    "    sim_neg=0\n",
    "    sim_pos=0\n",
    "\n",
    "    for attr in neg_attr:\n",
    "      sim_neg+=self.report[\"dev\"][np.where(self.collumns==attr)[0][0]]\n",
    "    if len(neg_attr) != 0:\n",
    "      sim_neg=(sim_neg/len(neg_attr)+1)/2#no%\n",
    "    \n",
    "    \n",
    "    for attr in pos_attr:\n",
    "        sim_pos+=self.report[\"dev\"][np.where(self.collumns==attr)[0][0]]\n",
    "    if len(pos_attr) != 0:\n",
    "      sim_pos=(sim_pos/len(pos_attr)+1)/2#no%\n",
    "    \n",
    "\n",
    "    if data.flag:\n",
    "      sim_pos=1-sim_pos#no%\n",
    "    else:\n",
    "      sim_neg=1-sim_neg#no%\n",
    "    self.sim=(sim_neg+sim_pos)/2\n",
    "    ######print(\"similarity\",self.sim) \n",
    "\n",
    "  # Relative deviation of cluster's accuracy\n",
    "  def acc_deviation(self,total_acc,min_acc,max_acc):\n",
    "    relative_deviation_acc=total_acc-self.report[\"acc\"]\n",
    "    \n",
    "    self.acc_ratio=(relative_deviation_acc-min_acc)/(max_acc-min_acc)#no%*100\n",
    "  \n",
    "  # Get components to cluster score\n",
    "  def get_score(self):\n",
    "    #self.score=(self.mean_deviation+self.ratio_extreme_deviation+self.adversial_distance+self.sim+self.acc_ratio)/5\n",
    "    self.scoredf=pd.DataFrame({\"acc_ratio\":self.acc_ratio,\"mean_deviation\":self.mean_deviation,\"ratio_extreme_dev\":self.ratio_extreme_deviation,\"adversial_distance\":self.adversial_distance,\"sim\":self.sim},index=[0]) # self.inv_amount_ratio+\n",
    "    ######print(\"FINAL\",self.score,\"\\n\")\n",
    "    return self.scoredf"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "bAHV2upahxxq"
   },
   "source": [
    "## Model classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "id": "DHRAIDTPM3fg"
   },
   "outputs": [],
   "source": [
    "class Model(ABC):\n",
    " \n",
    "  def __init__(self,strict=False):\n",
    "    self.result=None\n",
    "    self.size=0\n",
    "    self.data=None\n",
    "    self.clusters={}\n",
    "    self.classifier=None\n",
    "    self.score=[]\n",
    "    self.strict=strict\n",
    "\n",
    "  # Abstract methods for individual models\n",
    "  @abstractmethod\n",
    "  def train(self):\n",
    "    pass\n",
    "  \n",
    "  @abstractmethod\n",
    "  def predict(self,data):\n",
    "    pass\n",
    "\n",
    "  @abstractmethod\n",
    "  def perf(self):\n",
    "    pass\n",
    "\n",
    "  @abstractmethod\n",
    "  def load(self):\n",
    "    pass\n",
    "\n",
    "  @abstractmethod\n",
    "  def save(self):\n",
    "    pass\n",
    "\n",
    "  # Dvide data to clusters\n",
    "  def divide_data(self):\n",
    "  \n",
    "    # init dictionary for individual clusters\n",
    "    for x in range(0,self.size):\n",
    "      self.clusters[f'cluster{x}']=Cluster()\n",
    "      #self.report[f'cluster{x}']={}\n",
    "\n",
    "    # insert clustered data to predicted cluster\n",
    "    for i,row in enumerate(self.data.X.to_numpy()):\n",
    "      self.clusters[f'cluster{self.result.values[i][0]}'].add_data(row)\n",
    "\n",
    "  # Compute statistical metrics for each cluster\n",
    "  def statistical_report(self):\n",
    "    s = pd.Series([self.data.flag])\n",
    "    original_mean=self.data.X.mean()\n",
    "    original_min=self.data.X.min()\n",
    "    original_max=self.data.X.max()\n",
    "   \n",
    "    self.data.get_corr_analysis()\n",
    "    self.data.get_corr_pairs()\n",
    "    \n",
    "\n",
    "    for cluster in self.clusters:\n",
    "      # number of cluster\n",
    "      ######print(color.BOLD + color.RED+ cluster+ color.END)\n",
    "\n",
    "      # dataframe for effective metrics computation\n",
    "      self.clusters[cluster].data_to_array(self.data.X.columns)#self.data.X.columns\n",
    "      \n",
    "      # classify cluster\n",
    "      self.clusters[cluster].classify_cluster(self.classifier,s)\n",
    "\n",
    "      # variance of features\n",
    "      #self.clusters[cluster].variance()\n",
    "    \n",
    "      # average of features\n",
    "      self.clusters[cluster].get_mean()\n",
    "      \n",
    "      #difference of features\n",
    "      self.clusters[cluster].diff(original_mean,original_min,original_max)\n",
    "      \n",
    "      # correlation of features\n",
    "      self.clusters[cluster].correlation()\n",
    "\n",
    "      # size of cluster\n",
    "      self.clusters[cluster].size()\n",
    "  \n",
    "  # Compute components of cluster score\n",
    "  def compute_score(self):\n",
    "    accs_dev=list(map(lambda x: self.data.total_acc[self.class_number]-x[1].report[\"acc\"],list(self.clusters.items())))\n",
    "    max_acc=max(accs_dev)\n",
    "    min_acc=min(accs_dev)\n",
    "    self.score_elements=pd.DataFrame()\n",
    "    self.amount=[]\n",
    "    \n",
    "\n",
    "    #print(self.report['cluster0'][\"dev\"])\n",
    "    for cluster in self.clusters:\n",
    "\n",
    "      ######print(cluster)    \n",
    "      # amount of data in cluster\n",
    "      self.clusters[cluster].inv_amount(self.data.X.shape[0])\n",
    "\n",
    "      # pick up most deviated clusters \n",
    "      self.clusters[cluster].diff_deviation()\n",
    "      \n",
    "      # check if negativly correlated attrr are adversarial\n",
    "      self.clusters[cluster].adversial(self.data)\n",
    "      \n",
    "      #similarity to another class\n",
    "      self.clusters[cluster].similarity(self.data)\n",
    "\n",
    "      # acc deviation\n",
    "      self.clusters[cluster].acc_deviation(self.data.total_acc[self.class_number],min_acc,max_acc)\n",
    "      \n",
    "      # make one metrics, always percenatge, complement of acc, maybe normalize to 0,1 by averaging\n",
    "      self.score_elements=pd.concat([self.score_elements,self.clusters[cluster].get_score()])\n",
    "      self.amount.append(self.clusters[cluster].amount_ratio)\n",
    "    \n",
    "    self.score_elements.reset_index(inplace=True,drop=True)\n",
    "    #print(self.score_elements)\n",
    "    \n",
    "\n",
    "    # make one metrics, always percenatge, complement of acc, maybe normalize to 0,1 by averaging\n",
    "    self.final_score()\n",
    "    self.score_data=self.get_results()\n",
    "  \n",
    "  # Remove constant and uncorrelated components from score\n",
    "  def final_score(self):\n",
    "    #print(self.score_elements)\n",
    "    corr=self.score_elements.drop(columns=['acc_ratio']).corrwith(self.score_elements['acc_ratio'],method=\"spearman\").reset_index()\n",
    "    corr.columns=[\"atr\",\"corr\"]\n",
    "    #print(corr)\n",
    "    without_na=corr.fillna(value=-1)\n",
    "    drop=without_na[without_na['corr']<=0 ]['atr'].to_list()\n",
    "    #print(drop)\n",
    "    final_elements=self.score_elements.drop(columns=drop)\n",
    "    #print(final_elements)\n",
    "    final_elements['score']=final_elements.mean(axis=1)\n",
    "    #print(final_elements)\n",
    "    self.score=final_elements['score'].fillna(value=1)\n",
    "    #self.score=self.score_elements.drop(columns=['acc_ratio']).mean(axis=1)\n",
    "   \n",
    "  \n",
    "  # Normalize score and get weight vector\n",
    "  def get_results(self):\n",
    "    \n",
    "    acc_dev=[]\n",
    "    amount=[]\n",
    "    coeff=[]\n",
    "    relative_amount=[]\n",
    "    acc=[]\n",
    "\n",
    "    # 2sigma threshold\n",
    "    #print(self.score)\n",
    "    #print([self.score.mean() - 2 * self.score.std(), self.score.mean() + 2 * self.score.std()])\n",
    "    two_sigma_threshold=self.score.mean() + 2 * self.score.std()\n",
    "    if self.score.max() > two_sigma_threshold:\n",
    "      #print(\"APPLY 2SIGMA\")\n",
    "      sorted_score=self.score.sort_values(ascending=False).reset_index(drop=True)\n",
    "      #print(sorted_score)\n",
    "      max_score=sorted_score[1]\n",
    "      second=sorted_score[2]\n",
    "      #print(max_score)\n",
    "    else:\n",
    "      max_score=max(self.score)\n",
    "      second=max_score\n",
    "    \n",
    "    min_score=min(self.score)\n",
    "    interval_score=max_score-min_score\n",
    "    min_amount=min(self.amount)\n",
    "    max_amount=max(self.amount)\n",
    "    interval_amount=max_amount-min_amount\n",
    "    for cluster,score in zip(self.clusters,self.score):\n",
    "      \n",
    "      # relative score\n",
    "      if interval_score == 0:\n",
    "        coeff.append(1)\n",
    "      else:\n",
    "        rel_score=(score-min_score)/(interval_score)\n",
    "        if rel_score == 1:\n",
    "          #print(((second-min_score)/(interval_score)+1)/2)\n",
    "          rel_score=((second-min_score)/(interval_score)+1)/2\n",
    "        else:\n",
    "          rel_score=min(rel_score,1)\n",
    "        if self.strict:\n",
    "          new_score=rel_score*0.8+0.2\n",
    "        else:\n",
    "          new_score=rel_score/2+0.5\n",
    "\n",
    "\n",
    "        # relative amount\n",
    "        #rel_amount=(self.clusters[cluster].amount_ratio-min_amount)/(interval_amount*2)+0.5\n",
    "        #relative_amount.append(rel_amount)\n",
    "        \n",
    "        coeff.append(new_score)\n",
    "      \n",
    "      \n",
    "      amount.append( self.clusters[cluster].amount_ratio)\n",
    "      #red.append(self.clusters[cluster].report['red'])\n",
    "      acc.append(self.clusters[cluster].report['acc'])\n",
    "      acc_dev.append(self.clusters[cluster].acc_ratio)\n",
    "      \n",
    "    #return pd.DataFrame({\"score\":self.score,\"relative score\":relative_score,\"amount_rel\":relative_amount,\"coeff\":coeff,\"podil dat\":amount,\"inv_acc\":acc_dev,\"acc\":acc})\n",
    "    return pd.DataFrame({\"score\":self.score,\"weight\":coeff,\"amount\":amount})\n",
    "\n",
    "  # Plot results of score computation \n",
    "  def plot_results(self,description):\n",
    "    #self.score_elements.plot.bar(figsize=(9,7))\n",
    "    #plt.show()\n",
    "    #plt.clf()\n",
    "    #sns.regplot(x=\"acc_ratio\",y=\"mean_deviation\",data=self.score_elements,ci=None)\n",
    "    #plt.show()\n",
    "\n",
    "    print(\"TOTAL ACC\",self.data.total_acc[self.class_number])\n",
    "    ax=self.score_data.plot.bar(figsize=(9,7))\n",
    "    #ax.set(xlabel='Clusters', ylabel='Value',fontsize=15)\n",
    "    plt.xlabel('Shluky', fontsize=15)\n",
    "    plt.ylabel('Hodnota', fontsize=15)\n",
    "    ax.set_xticklabels(ax.get_xticklabels(), rotation=45)\n",
    "    ax.set_title(description,fontsize=20)\n",
    "    fig=ax.get_figure()\n",
    "    plt.show()\n",
    "    plt.clf()\n",
    "    #ax=sns.regplot(x=\"score\",y=\"inv_acc\",data=self.score_data,ci=None)\n",
    "    #plt.show()\n",
    "    #fig.savefig(f\"xsetin00/{self.data.use_case}/{description}.png\",dpi=500)\n",
    "  \n",
    "  # Save weight vector for next step\n",
    "  def save_report(self):\n",
    "    self.coeffs=self.score_data['weight']\n",
    "    self.coeffs=self.coeffs.rename(\"coeffs\")\n",
    "    #print(self.coeffs)\n",
    "\n",
    "  # Run whole analysis\n",
    "  def cluster_analysis(self,pca=False,description=\"None description\"):\n",
    "    if pca:\n",
    "      self.prep=make_pipeline(StandardScaler(),VarianceThreshold(),PCA(n_components='mle'))\n",
    "    else:\n",
    "      self.prep=make_pipeline(StandardScaler(),VarianceThreshold())\n",
    "    self.pca=pca\n",
    "    self.train()\n",
    "    self.perf()\n",
    "    self.divide_data()\n",
    "    self.statistical_report()\n",
    "    self.compute_score()\n",
    "    self.plot_results(description)\n",
    "    self.save_report()\n",
    "\n",
    "# Class for H2O framework\n",
    "class H2O_model(Model):\n",
    "\n",
    "  def __init__(self, data, classifier,class_number,strict=False):\n",
    "    super().__init__(strict)\n",
    "    self.data=data\n",
    "    self.classifier=classifier\n",
    "    self.class_number=class_number\n",
    "    self.data.set_class(class_number)\n",
    "\n",
    "    # model inicializaton\n",
    "    self.model=H2OKMeansEstimator( k=10,\n",
    "                              estimate_k=True,\n",
    "                              standardize=False,\n",
    "                              max_iterations = 100)#ranmagic\n",
    "  \n",
    "  # Train H2O cluster model\n",
    "  def train(self):\n",
    "    # create h2o data frame\n",
    "    inner_data = h2o.H2OFrame(self.prep.fit_transform(self.data.X.to_numpy()))\n",
    "    \n",
    "    # create model for kmeans algorithm\n",
    "    self.model.train(training_frame=inner_data)\n",
    "\n",
    "    self.size=len(self.model.size())\n",
    "\n",
    "    tmp=self.model.predict(inner_data)\n",
    "    self.result=tmp.as_data_frame(use_pandas=True, header=True)\n",
    "    self.centers=self.model.centers().as_data_frame(use_pandas=True, header=True)\n",
    "  \n",
    "  # Cluster data\n",
    "  def predict(self,data):\n",
    "    if data is self.data:\n",
    "      #print(\"SAME DATA\")\n",
    "      self.result_new=self.result\n",
    "    else:\n",
    "      h2o_frame=h2o.H2OFrame(self.prep.fit_transform(data.X))\n",
    "      tmp=self.model.predict(h2o_frame)\n",
    "      self.result_new=tmp.as_data_frame(use_pandas=True, header=True)\n",
    "   \n",
    "  # Show model metrics\n",
    "  def perf(self):\n",
    "    print(self.model.model_performance())\n",
    "  \n",
    "  # Save model\n",
    "  def save(self,path):\n",
    "    h2o.save_model(model=self.model, path=path, force=True)\n",
    "  \n",
    "  # Load model\n",
    "  def load(self,path):\n",
    "    self.model=h2o.load_model(path)\n",
    "\n",
    "# Class for Sklearn framework\n",
    "class Sklearn_model(Model):\n",
    "\n",
    "  def __init__(self, data, classifier,class_number,model='kmeans',index='bouldin',strict=False):\n",
    "    super().__init__(strict)\n",
    "    self.data=data\n",
    "    self.classifier=classifier\n",
    "    self.class_number=class_number\n",
    "    self.data.set_class(class_number)\n",
    "    self.model_name=model\n",
    "    self.max_k=10\n",
    "    self.optimal_ks={}\n",
    "    self.index=index\n",
    "    self.criterion =min(0.02 + 10. / self.data.X.shape[0] + 2.5 / pow(self.data.X.shape[1], 2), 0.8)\n",
    "\n",
    "    # Models initialization\n",
    "    if self.model_name == 'kmeans':\n",
    "      self.models=[KMeans(n_clusters=i,max_iter=100,n_init=\"auto\") for i in range(1,self.max_k+1)]#ranmagic\n",
    "    elif self.model_name == 'mean_shift':\n",
    "       self.models=[MeanShift()]\n",
    "    elif self.model_name == 'aglomerative':\n",
    "       self.models=[AgglomerativeClustering(n_clusters=i, linkage=\"ward\") for i in range(1,self.max_k+1)]\n",
    "\n",
    "  # Visualize methods for K estimation\n",
    "  def visualize(self,data):\n",
    "    #metrics=['calinski_harabasz','silhouette','elbow']\n",
    "\n",
    "    model = MiniBatchKMeans(max_iter=100,n_init=\"auto\")#ranmagic\n",
    "    # k is range of number of clusters.\n",
    "    if self.index == 'elbow':\n",
    "      visualizer = KElbowVisualizer(model, k=(3,10), timings=False )\n",
    "    else:\n",
    "      visualizer = KElbowVisualizer(model, k=(3,10) ,metric=self.index, timings=False )\n",
    "    # Fit the data to the visualizer\n",
    "    visualizer.fit(data.copy())\n",
    "            \n",
    "    visualizer.show()#outpath=f\"xsetin00/{self.data.use_case}/{self.index}{self.class_number}.pdf\")  \n",
    "    self.optimal_ks[self.index]=visualizer.elbow_value_\n",
    "    #plt.savefig(f\"xsetin00/{self.data.use_case}/{self.index}.png\",dpi=100)\n",
    "    plt.show()\n",
    "    plt.style.use('default')\n",
    "\n",
    "  # K estimation methods with iteration stop criterion\n",
    "  def pre(self,data):\n",
    "    models=[MiniBatchKMeans(n_clusters=i,max_iter=100,n_init=\"auto\") for i in range(1,self.max_k+1)]#ranmagic\n",
    "    model_old = models[0]\n",
    "    model_old.fit(data)\n",
    "   \n",
    "    \n",
    "    distorsions = []\n",
    "    bouldin_score=[]\n",
    "    if self.index=='PRE':\n",
    "        print(\"CRIT\",self.criterion)\n",
    "        distorsions.append(model_old.inertia_)\n",
    "        print(\"SSE\",model_old.inertia_)\n",
    "    k=2\n",
    "    for model in models[1:]:\n",
    "        model.fit(data)\n",
    "        bouldin_score.append(davies_bouldin_score(data, model.labels_))\n",
    "        if self.index=='PRE':\n",
    "          distorsions.append(model.inertia_)\n",
    "          print(\"SSE\",model.inertia_)\n",
    "          PRE=(distorsions[k-2]-distorsions[k-1])/distorsions[k-2]\n",
    "          print(\"PRE\",PRE,k)\n",
    "          k+=1\n",
    "          if PRE < self.criterion:\n",
    "            break\n",
    "        model_old=model\n",
    "    \n",
    "    sorted=bouldin_score.copy()\n",
    "    sorted.sort()\n",
    "    self.optimal_ks['bouldin']=bouldin_score.index(sorted[0])+2\n",
    "    if self.optimal_ks['bouldin'] == 2:\n",
    "      self.optimal_ks['bouldin']=bouldin_score.index(sorted[1])+2\n",
    "\n",
    "    if self.index=='PRE':\n",
    "        self.optimal_ks['PRE']=len(np.unique(model_old.labels_)) \n",
    "        fig = plt.figure(figsize=(15, 5))\n",
    "        plt.plot(range(1, len(distorsions)+1), distorsions)\n",
    "        plt.grid(True)\n",
    "        plt.title('Elbow curve')\n",
    "        plt.show()\n",
    "    else:\n",
    "        plt.plot(range(2, len(bouldin_score)+2), bouldin_score, linestyle='--', marker='o', color='b');\n",
    "        plt.xlabel('K');\n",
    "        plt.ylabel('Davies Bouldin score');\n",
    "        plt.title('Davies Bouldin score vs. K');\n",
    "        #plt.savefig(f\"xsetin00/{self.data.use_case}/bouldin{self.class_number}.pdf\")\n",
    "        plt.show()\n",
    "\n",
    "  # Estimate K with demanded method\n",
    "  def estimate_k(self,data):\n",
    "    if self.index in ['PRE','bouldin']:\n",
    "      self.pre(data)\n",
    "    else:\n",
    "      self.visualize(data)\n",
    "    \n",
    "    self.size=self.optimal_ks[self.index]\n",
    "\n",
    "  # Train Sklearn model\n",
    "  def train(self):\n",
    "    print(self.data.X.shape)\n",
    "    prep_data=self.prep.fit_transform(self.data.X)\n",
    "    print(prep_data.shape)\n",
    "    self.estimate_k(prep_data)\n",
    "    self.model=self.models[self.size-1]\n",
    "    self.model.fit(prep_data)\n",
    "    self.result=pd.DataFrame(self.model.labels_,columns=[\"predict\"])\n",
    "    self.centers=self.model.cluster_centers_\n",
    "\n",
    "  # Cluster data \n",
    "  def predict(self,data):\n",
    "    if self.model_name=='aglomerative' or data is self.data:\n",
    "        self.result_new=self.result\n",
    "    else:\n",
    "        self.result_new=pd.DataFrame(self.model.predict(self.prep.fit_transform(data.X)),columns=[\"predict\"])\n",
    "  \n",
    "  # Show model metrics\n",
    "  def perf(self):\n",
    "    print(\"Final K parameter:\",self.size)\n",
    "    if self.index=='PRE':\n",
    "      print(\"SSE\",self.model.inertia_)\n",
    "  \n",
    "  # Save model\n",
    "  def save(self,path):\n",
    "    dump(self.model,path)\n",
    "  \n",
    "  # Load model\n",
    "  def load(self,path):\n",
    "    self.model=load(path)\n",
    "  \n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset reduction and merging"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For saving results \n",
    "framework_result=pd.DataFrame()\n",
    "output_directory=\"xsetin00/Data/reduced_datasets/averaged/\"#\"../data/reduced_datasets/\"#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Description of experiment\n",
    "text='reduction'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define context of applied use case\n",
    "\n",
    "def prep_data(use_case,data,label):\n",
    "        class_values=data[1]#(True,False)#(\"dga\",'legit')\n",
    "\n",
    "        # datasets paths\n",
    "        old_data_path = f\"xsetin00/Data/refdata/{data[0]}.csv\"#f\"../refdata/{data[0]}.csv\"#f\"xsetin00/Data/refdata/{data[0]}.csv\"#\"REFDATA/data/\n",
    "\n",
    "        # old data init\n",
    "        old_data=Modifier(f\"{old_data_path}\",label,use_case,class_values,to_drop=[])\n",
    "        old_data.load()\n",
    "        classcol=old_data.csv_data.pop(\"Class\")\n",
    "        cat=old_data.csv_data.select_dtypes(exclude=['floating'])\n",
    "        num=old_data.csv_data.select_dtypes(include=['floating'])\n",
    "        scaler=StandardScaler()\n",
    "        if not num.empty:\n",
    "                num=pd.DataFrame(scaler.fit_transform(num),columns=num.columns)\n",
    "        \n",
    "        label_encoder = LabelEncoder()\n",
    "        encoded=cat.apply(label_encoder.fit_transform)\n",
    "        dfs = [num, encoded, classcol]\n",
    "        non_empt_dfs = [df for df in dfs if not df.empty]\n",
    "        old_data.csv_data=pd.concat(non_empt_dfs,axis=1)\n",
    "        # from sklearn.utils import shuffle\n",
    "        # old_data.csv_data=shuffle(old_data.csv_data,random_state=42)\n",
    "        #old_data.save(f\"xsetin00/Data/refdata_prepared/{data[0]}.csv\")\n",
    "        \n",
    "        print(old_data.class_balance(old_data.csv_data[label]))\n",
    "      \n",
    "        folds=5\n",
    "        kf = StratifiedKFold(n_splits=folds,shuffle=True,random_state=None)\n",
    "        indexs=[]\n",
    "        for train_index, test_index in kf.split(old_data.get_all_features(), old_data.get_all_label()): \n",
    "                indexs.append((train_index,test_index))\n",
    "\n",
    "        test_data=Dataset(None,label,use_case,class_values)\n",
    "\n",
    "     \n",
    "\n",
    "        return old_data,test_data,indexs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_models(train_data,test_data,use_case,dumb_ratio=0.99,knn=1,model_type=\"knn\"):\n",
    "    # Creating and eventually training classifier\n",
    "    clf_types=['old','dumb','clustered','compare']\n",
    "    clfs={}  \n",
    "    result=pd.DataFrame()\n",
    "    for clf_type in clf_types:\n",
    "\n",
    "        #clfs[clf_type] = DecisionTreeClassifier(random_state=42)\n",
    "        if model_type==\"knn\":\n",
    "            clfs[clf_type]=KNeighborsClassifier(n_neighbors=knn)#KNeighborsClassifier(n_neighbors=knn) #AdaBoostClassifier(\n",
    "                # estimator=DecisionTreeClassifier(max_depth=6,criterion='entropy',random_state=42,max_features=None),n_estimators=5,algorithm='SAMME.R',random_state=42\n",
    "                # )\n",
    "        elif model_type==\"tree\":\n",
    "            clfs[clf_type]=DecisionTreeClassifier(random_state=42)\n",
    "            \n",
    "        if clf_type == 'old':\n",
    "            clfs[clf_type].fit(train_data.get_all_features(), train_data.get_all_label())\n",
    "            pred=clfs[clf_type].predict(test_data.get_all_features())\n",
    "            report=classification_report(test_data.get_all_label(),pred,digits=4,output_dict=True)\n",
    "            train_data.cnt_new=train_data.class_balance(train_data.get_all_label())\n",
    "            train_data.improvement=0\n",
    "            tmp=train_data.prepare_model_result(report,text+\" 1\",use_case,clf_type+\" \"+str(knn))\n",
    "            result=pd.concat([result,tmp])\n",
    "        elif clf_type == 'dumb':\n",
    "            # just small sample to train classifier\n",
    "            dumb_X_train, _, dumb_y_train, _ = train_test_split(train_data.get_all_features(), train_data.get_all_label(), test_size=dumb_ratio, random_state=42, shuffle=True, stratify=train_data.get_all_label())#0.99#0.8\n",
    "            clfs[clf_type].fit(dumb_X_train, dumb_y_train)\n",
    "        else:\n",
    "            continue    \n",
    "\n",
    "        pred=clfs[clf_type].predict(test_data.get_all_features())\n",
    "        print(clf_type,classification_report(test_data.get_all_label(),pred,digits=4))\n",
    "    # Get overall accuracy of classifier for each class\n",
    "    acc_vector=[(train_data,clfs[\"dumb\"],0),(train_data,clfs[\"dumb\"],1)]\n",
    "    #Parallel(n_jobs=1,backend=\"threading\")([delayed(data[0].dataset_acc)(data[1],data[2]) for data in acc_vector])\n",
    "    [data[0].dataset_acc(data[1],data[2]) for data in acc_vector]\n",
    "    return clfs,result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "ParserError",
     "evalue": "Error tokenizing data. C error: Calling read(nbytes) on source failed. Try engine='python'.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mParserError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[38], line 14\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[39m# old data init\u001b[39;00m\n\u001b[1;32m     13\u001b[0m old_data\u001b[39m=\u001b[39mModifier(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mold_data_path\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m,label,use_case,class_values,to_drop\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39mdomain\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m---> 14\u001b[0m old_data\u001b[39m.\u001b[39;49mload()\n\u001b[1;32m     15\u001b[0m \u001b[39mprint\u001b[39m(use_case,\u001b[39m\"\u001b[39m\u001b[39mold data\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     16\u001b[0m \u001b[39mprint\u001b[39m(old_data\u001b[39m.\u001b[39mclass_balance(old_data\u001b[39m.\u001b[39mcsv_data[label]))\n",
      "Cell \u001b[0;32mIn[4], line 22\u001b[0m, in \u001b[0;36mDataset.load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mload\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m---> 22\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcsv_data\u001b[39m=\u001b[39mpd\u001b[39m.\u001b[39;49mread_csv(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpath)\n\u001b[1;32m     23\u001b[0m   label_encoder \u001b[39m=\u001b[39m LabelEncoder()\n\u001b[1;32m     24\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcsv_data[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlabel] \u001b[39m=\u001b[39m label_encoder\u001b[39m.\u001b[39mfit_transform(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcsv_data[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlabel])\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py:912\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    899\u001b[0m kwds_defaults \u001b[39m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    900\u001b[0m     dialect,\n\u001b[1;32m    901\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    908\u001b[0m     dtype_backend\u001b[39m=\u001b[39mdtype_backend,\n\u001b[1;32m    909\u001b[0m )\n\u001b[1;32m    910\u001b[0m kwds\u001b[39m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 912\u001b[0m \u001b[39mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py:583\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    580\u001b[0m     \u001b[39mreturn\u001b[39;00m parser\n\u001b[1;32m    582\u001b[0m \u001b[39mwith\u001b[39;00m parser:\n\u001b[0;32m--> 583\u001b[0m     \u001b[39mreturn\u001b[39;00m parser\u001b[39m.\u001b[39;49mread(nrows)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py:1704\u001b[0m, in \u001b[0;36mTextFileReader.read\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1697\u001b[0m nrows \u001b[39m=\u001b[39m validate_integer(\u001b[39m\"\u001b[39m\u001b[39mnrows\u001b[39m\u001b[39m\"\u001b[39m, nrows)\n\u001b[1;32m   1698\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1699\u001b[0m     \u001b[39m# error: \"ParserBase\" has no attribute \"read\"\u001b[39;00m\n\u001b[1;32m   1700\u001b[0m     (\n\u001b[1;32m   1701\u001b[0m         index,\n\u001b[1;32m   1702\u001b[0m         columns,\n\u001b[1;32m   1703\u001b[0m         col_dict,\n\u001b[0;32m-> 1704\u001b[0m     ) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_engine\u001b[39m.\u001b[39;49mread(  \u001b[39m# type: ignore[attr-defined]\u001b[39;49;00m\n\u001b[1;32m   1705\u001b[0m         nrows\n\u001b[1;32m   1706\u001b[0m     )\n\u001b[1;32m   1707\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m:\n\u001b[1;32m   1708\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclose()\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/c_parser_wrapper.py:234\u001b[0m, in \u001b[0;36mCParserWrapper.read\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    233\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlow_memory:\n\u001b[0;32m--> 234\u001b[0m         chunks \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_reader\u001b[39m.\u001b[39;49mread_low_memory(nrows)\n\u001b[1;32m    235\u001b[0m         \u001b[39m# destructive to chunks\u001b[39;00m\n\u001b[1;32m    236\u001b[0m         data \u001b[39m=\u001b[39m _concatenate_chunks(chunks)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/_libs/parsers.pyx:812\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/_libs/parsers.pyx:873\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/_libs/parsers.pyx:848\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/_libs/parsers.pyx:859\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._check_tokenize_status\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/_libs/parsers.pyx:2025\u001b[0m, in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mParserError\u001b[0m: Error tokenizing data. C error: Calling read(nbytes) on source failed. Try engine='python'."
     ]
    }
   ],
   "source": [
    "# Define context of applied use case\n",
    "use_case=\"DOH\"\n",
    "label=\"is_doh\"#class\n",
    "class_values=(True,False)#(True,False)#(\"dga\",'legit')\n",
    "merging=True # if true, new data is merged with old data, if false, old data is just reduced\n",
    "\n",
    "# datasets paths\n",
    "old_data_path = \"/srv/data/pesekja8/datasets/generated_data.csv\"#\"/srv/data/pesekja8/datasets/generated_data.csv\"#\"xsetin00/DGA/train_dataset.csv\"#\"xsetin00/doh_test_analyze_3M.csv\"#\"/srv/data/pesekja8/datasets/generated_data.csv\" # \"/srv/data/pesekja8/datasets/doh_paper.csv\"\n",
    "new_data_path = \"xsetin00/doh_test_analyze_3M.csv\"\n",
    "test_size=0.2\n",
    "\n",
    "# old data init\n",
    "old_data=Modifier(f\"{old_data_path}\",label,use_case,class_values,to_drop=['domain'])\n",
    "old_data.load()\n",
    "print(use_case,\"old data\")\n",
    "print(old_data.class_balance(old_data.csv_data[label]))\n",
    "X_train_old, X_test_old, y_train_old, y_test_old = train_test_split(old_data.get_all_features(), old_data.get_all_label(), test_size=test_size, random_state=42, shuffle=True, stratify=old_data.get_all_label())#995\n",
    "old_data.set_data(pd.concat([X_train_old,y_train_old],axis=1))\n",
    "print(old_data.class_balance(old_data.csv_data[label]))\n",
    "\n",
    "# new data init\n",
    "if merging:\n",
    "    new_data=Modifier(f\"{new_data_path}\",label,use_case,class_values,to_drop=['Dataset'])\n",
    "    new_data.load()\n",
    "    print(use_case,\"new data\")\n",
    "    print(new_data.class_balance(new_data.csv_data[label]))\n",
    "    X_train_new, X_test_new, y_train_new, y_test_new = train_test_split(new_data.get_all_features(), new_data.get_all_label(), test_size=test_size, random_state=42, shuffle=True, stratify=new_data.get_all_label())\n",
    "    new_data.set_data(pd.concat([X_train_new,y_train_new],axis=1))\n",
    "    print(new_data.class_balance(new_data.csv_data[label]))\n",
    "else:\n",
    "    X_test_new, y_test_new = pd.DataFrame(),pd.DataFrame()\n",
    "    \n",
    "#old_data.set_data(pd.concat([old_data.csv_data,new_data.csv_data],axis=0))\n",
    "#print(\"just reduction\",old_data.class_balance(old_data.csv_data[label]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>At1</th>\n",
       "      <th>At2</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3248</th>\n",
       "      <td>-0.0753</td>\n",
       "      <td>0.267</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4463</th>\n",
       "      <td>0.9240</td>\n",
       "      <td>0.861</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2556</th>\n",
       "      <td>-0.4840</td>\n",
       "      <td>0.615</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4348</th>\n",
       "      <td>0.8690</td>\n",
       "      <td>0.576</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3704</th>\n",
       "      <td>1.3700</td>\n",
       "      <td>1.450</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2708</th>\n",
       "      <td>-0.9920</td>\n",
       "      <td>1.400</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4446</th>\n",
       "      <td>0.9410</td>\n",
       "      <td>1.460</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2340</th>\n",
       "      <td>0.8200</td>\n",
       "      <td>-0.514</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>274</th>\n",
       "      <td>1.3300</td>\n",
       "      <td>1.420</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3452</th>\n",
       "      <td>-0.6520</td>\n",
       "      <td>-1.540</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1060 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         At1    At2  Class\n",
       "3248 -0.0753  0.267      1\n",
       "4463  0.9240  0.861      0\n",
       "2556 -0.4840  0.615      1\n",
       "4348  0.8690  0.576      0\n",
       "3704  1.3700  1.450      1\n",
       "...      ...    ...    ...\n",
       "2708 -0.9920  1.400      0\n",
       "4446  0.9410  1.460      0\n",
       "2340  0.8200 -0.514      1\n",
       "274   1.3300  1.420      1\n",
       "3452 -0.6520 -1.540      0\n",
       "\n",
       "[1060 rows x 3 columns]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating testing data\n",
    "if merging:\n",
    "    X_test=pd.concat([X_test_old,X_test_new])\n",
    "    y_test=pd.concat([y_test_old,y_test_new])   \n",
    "else:\n",
    "    X_test=X_test_old\n",
    "    y_test=y_test_old\n",
    "test_data=Dataset(_,label,use_case,class_values)\n",
    "test_data.set_data(pd.concat([X_test,y_test],axis=1))\n",
    "test_data.csv_data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifiers training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "old               precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8669    0.8684    0.8676       585\n",
      "           1     0.8376    0.8358    0.8367       475\n",
      "\n",
      "    accuracy                         0.8538      1060\n",
      "   macro avg     0.8522    0.8521    0.8522      1060\n",
      "weighted avg     0.8537    0.8538    0.8538      1060\n",
      "\n",
      "dumb               precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8038    0.8615    0.8317       585\n",
      "           1     0.8129    0.7411    0.7753       475\n",
      "\n",
      "    accuracy                         0.8075      1060\n",
      "   macro avg     0.8084    0.8013    0.8035      1060\n",
      "weighted avg     0.8079    0.8075    0.8064      1060\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Creating and eventually training classifier\n",
    "if merging:\n",
    "    clf_types=['old','dumb','clustered','compare','new']\n",
    "else:\n",
    "    clf_types=['old','dumb','clustered','compare']\n",
    "clfs={}  \n",
    "for clf_type in clf_types:\n",
    "\n",
    "    #clfs[clf_type] = DecisionTreeClassifier(random_state=42)\n",
    "    clfs[clf_type]=KNeighborsClassifier(n_neighbors=1)#AdaBoostClassifier(\n",
    "        # estimator=DecisionTreeClassifier(max_depth=6,criterion='entropy',random_state=42,max_features=None),n_estimators=5,algorithm='SAMME.R',random_state=42\n",
    "        # )\n",
    "    if clf_type == 'old':\n",
    "        clfs[clf_type].fit(X_train_old, y_train_old)\n",
    "    elif clf_type == 'dumb':\n",
    "        # just small sample to train classifier\n",
    "        dumb_X_train, _, dumb_y_train, _ = train_test_split(X_train_old, y_train_old, test_size=0.99, random_state=42, shuffle=True, stratify=y_train_old)#0.99#0.8\n",
    "        clfs[clf_type].fit(dumb_X_train, dumb_y_train)\n",
    "    elif clf_type == 'new':\n",
    "        clfs[clf_type].fit(X_train_new, y_train_new)\n",
    "    else:\n",
    "        continue    \n",
    "\n",
    "    pred=clfs[clf_type].predict(X_test)\n",
    "    print(clf_type,classification_report(y_test,pred,digits=4))\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classify individual classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Accuracy for class 0 is 0.8867037195382642',\n",
       " 'Accuracy for class 1 is 0.7958968963703315']"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get overall accuracy of classifier for each class\n",
    "if merging:\n",
    "    acc_vector=[(old_data,clfs[\"dumb\"],0),(old_data,clfs[\"dumb\"],1),(new_data,clfs[\"old\"],0),(new_data,clfs[\"old\"],1)]\n",
    "else:\n",
    "    acc_vector=[(old_data,clfs[\"dumb\"],0),(old_data,clfs[\"dumb\"],1)]\n",
    "#Parallel(n_jobs=1,backend=\"threading\")([delayed(data[0].dataset_acc)(data[1],data[2]) for data in acc_vector])\n",
    "[data[0].dataset_acc(data[1],data[2]) for data in acc_vector]\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classify on test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Dataset</th>\n",
       "      <th>Method</th>\n",
       "      <th>Metric</th>\n",
       "      <th>Acc</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Impr</th>\n",
       "      <th>Support_0</th>\n",
       "      <th>Support_1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DOH</td>\n",
       "      <td>merging</td>\n",
       "      <td>old</td>\n",
       "      <td>0.729436</td>\n",
       "      <td>0.767998</td>\n",
       "      <td>0.729436</td>\n",
       "      <td>0.692035</td>\n",
       "      <td>0</td>\n",
       "      <td>1352542</td>\n",
       "      <td>228729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DOH</td>\n",
       "      <td>merging</td>\n",
       "      <td>new</td>\n",
       "      <td>0.852488</td>\n",
       "      <td>0.866056</td>\n",
       "      <td>0.852488</td>\n",
       "      <td>0.854321</td>\n",
       "      <td>0</td>\n",
       "      <td>1352542</td>\n",
       "      <td>228729</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Dataset   Method Metric       Acc  Precision    Recall        F1  Impr   \n",
       "0     DOH  merging    old  0.729436   0.767998  0.729436  0.692035     0  \\\n",
       "0     DOH  merging    new  0.852488   0.866056  0.852488  0.854321     0   \n",
       "\n",
       "  Support_0 Support_1  \n",
       "0   1352542    228729  \n",
       "0   1352542    228729  "
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run classifier on testing data   \n",
    "for clf_type in [\"old\",'new']:#clf_types:\n",
    "    pred=clfs[clf_type].predict(X_test)\n",
    "    report=classification_report(y_test,pred,digits=4,output_dict=True)\n",
    "    old_data.cnt_new=old_data.class_balance(y_train_old)\n",
    "    old_data.improvement=0\n",
    "    tmp=old_data.prepare_model_result(report,text,use_case,clf_type)\n",
    "    framework_result=pd.concat([framework_result,tmp])\n",
    "framework_result    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply clustering"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analyze and extract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze and extract one class and one dataset at a time\n",
    "def modification_one(label,data,clf,undersample,metric='elbow',alg=\"kmeans\",desc=\"\"):\n",
    "    \n",
    "    # class reduction\n",
    "    if alg==\"h2o\":\n",
    "        cluster_model=H2O_model(data,clf,label)\n",
    "        \n",
    "    else:\n",
    "        cluster_model=Sklearn_model(data,clf,label,model=alg,index=metric)\n",
    "   \n",
    "    cluster_model.cluster_analysis(pca=True,description=desc+\" - \"+str(label))\n",
    "\n",
    "    \n",
    "    extracted_data=data.extract(cluster_model,undersample,oversample=True)\n",
    "    \n",
    "    #del cluster_model\n",
    "    return pd.DataFrame(extracted_data)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare input vector for clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to define parameters data modifcation\n",
    "def get_params(red=1,neg_label=None,pos_label=None,old_data=None,new_data=None,clfs=None,merging=False):\n",
    "    if pos_label is not None:\n",
    "        reduction=(pos_label+neg_label)/old_data.csv_data.shape[0]\n",
    "        tmp=neg_label/(pos_label+neg_label)\n",
    "        label_weights=[tmp,1-tmp]\n",
    "    else:\n",
    "        reduction=red\n",
    "        label_weights=[0.5,0.5]\n",
    "    if merging:\n",
    "        style=[('Old',old_data,clfs['dumb']),('New',new_data,clfs['old'])]\n",
    "    else:\n",
    "        style=[('Old',old_data,clfs['dumb'])]\n",
    "\n",
    "    input_vector=[]\n",
    "    for dataset_type,data,clf in style:\n",
    "        desc=dataset_type+\" data analysis\"\n",
    "        for label in [0,1]:\n",
    "            if len(style) == 1: # only one dataset\n",
    "                dataset_weights={\"Old\":1}\n",
    "            else:\n",
    "                #reduce_coeff=min(max(math.floor((new_data.total_acc[label]/old_data.total_acc[label])*10)/10,0.2),0.8)\n",
    "                reduce_coeff=min(max(new_data.total_acc[label]/(new_data.total_acc[label]+old_data.total_acc[label]),0.2),0.8)\n",
    "                # balance_old=old_data.class_balance()\n",
    "                # balance_new=new_data.class_balance()\n",
    "                # amount_coeff=balance_old[label]/(balance_new[label]+balance_old[label])\n",
    "                # dataset_weights={\"Old\":(reduce_coeff+amount_coeff)/2,\"New\": ((1-reduce_coeff)+(1-amount_coeff))/2}\n",
    "                dataset_weights={\"Old\":reduce_coeff,\"New\": 1-reduce_coeff}\n",
    "                #print(dataset_weights)\n",
    "\n",
    "            demanded_amount=int(old_data.csv_data.shape[0]*reduction*label_weights[label]*dataset_weights[dataset_type])#263545\n",
    "            print(desc+\" - \"+str(label),\"To be extracted - \",demanded_amount)\n",
    "            input_vector.append((label,data,clf,demanded_amount,desc))\n",
    "    return input_vector, 2*len(style), reduction"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Viszualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vizualize(train,test):\n",
    "    print(\"Train\")\n",
    "    print(train)\n",
    "    print(\"Test\")\n",
    "    print(test)\n",
    "\n",
    "    sns.pairplot(train,hue='Class')\n",
    "    # fig, ax = plt.subplots()\n",
    "    # print(train[:, 0])\n",
    "    # # Plot train dataset\n",
    "    # ax.scatter(train[:, 0], train[:, 1], c='blue', label='Train', marker='o')\n",
    "\n",
    "    # # Plot test dataset\n",
    "    # ax.scatter(test[:, 0], test[:, 1], c='red', label='Test', marker='s')\n",
    "\n",
    "    # # Set labels and title\n",
    "    # ax.set_xlabel('Feature 1')\n",
    "    # ax.set_ylabel('Feature 2')\n",
    "    # ax.set_title('Scatter Matrix')\n",
    "\n",
    "    # # Add legend\n",
    "    # ax.legend()\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run modification process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 2924, 1: 2376, 'all': 5300}\n",
      "old               precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8762    0.9197    0.8974       585\n",
      "           1     0.8946    0.8400    0.8664       475\n",
      "\n",
      "    accuracy                         0.8840      1060\n",
      "   macro avg     0.8854    0.8798    0.8819      1060\n",
      "weighted avg     0.8845    0.8840    0.8835      1060\n",
      "\n",
      "dumb               precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8865    0.9077    0.8970       585\n",
      "           1     0.8829    0.8568    0.8697       475\n",
      "\n",
      "    accuracy                         0.8849      1060\n",
      "   macro avg     0.8847    0.8823    0.8833      1060\n",
      "weighted avg     0.8849    0.8849    0.8847      1060\n",
      "\n",
      "Old data analysis - 0 To be extracted -  1060\n",
      "Old data analysis - 1 To be extracted -  1060\n",
      "{0: 1060, 1: 1060}\n",
      "IRB\n",
      "(2119, 2) 2120.0\n",
      "Old data analysis - 0 To be extracted -  848\n",
      "Old data analysis - 1 To be extracted -  848\n",
      "{0: 848, 1: 848}\n",
      "IRB\n",
      "(1695, 2) 1696.0\n",
      "Old data analysis - 0 To be extracted -  636\n",
      "Old data analysis - 1 To be extracted -  636\n",
      "{0: 636, 1: 636}\n",
      "IRB\n",
      "(1271, 2) 1272.0\n",
      "Old data analysis - 0 To be extracted -  424\n",
      "Old data analysis - 1 To be extracted -  424\n",
      "{0: 424, 1: 424}\n",
      "IRB\n",
      "(847, 2) 848.0\n",
      "Old data analysis - 0 To be extracted -  212\n",
      "Old data analysis - 1 To be extracted -  212\n",
      "{0: 212, 1: 212}\n",
      "IRB\n",
      "(423, 2) 424.0\n",
      "Old data analysis - 0 To be extracted -  106\n",
      "Old data analysis - 1 To be extracted -  106\n",
      "{0: 106, 1: 106}\n",
      "IRB\n",
      "(211, 2) 212.0\n",
      "old               precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8736    0.9094    0.8911       585\n",
      "           1     0.8825    0.8379    0.8596       475\n",
      "\n",
      "    accuracy                         0.8774      1060\n",
      "   macro avg     0.8780    0.8736    0.8754      1060\n",
      "weighted avg     0.8776    0.8774    0.8770      1060\n",
      "\n",
      "dumb               precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8963    0.9162    0.9062       585\n",
      "           1     0.8939    0.8695    0.8815       475\n",
      "\n",
      "    accuracy                         0.8953      1060\n",
      "   macro avg     0.8951    0.8929    0.8939      1060\n",
      "weighted avg     0.8953    0.8953    0.8951      1060\n",
      "\n",
      "Old data analysis - 0 To be extracted -  1060\n",
      "Old data analysis - 1 To be extracted -  1060\n",
      "{0: 1060, 1: 1060}\n",
      "IRB\n",
      "(2119, 2) 2120.0\n",
      "Old data analysis - 0 To be extracted -  848\n",
      "Old data analysis - 1 To be extracted -  848\n",
      "{0: 848, 1: 848}\n",
      "IRB\n",
      "(1695, 2) 1696.0\n",
      "Old data analysis - 0 To be extracted -  636\n",
      "Old data analysis - 1 To be extracted -  636\n",
      "{0: 636, 1: 636}\n",
      "IRB\n",
      "(1271, 2) 1272.0\n",
      "Old data analysis - 0 To be extracted -  424\n",
      "Old data analysis - 1 To be extracted -  424\n",
      "{0: 424, 1: 424}\n",
      "IRB\n",
      "(847, 2) 848.0\n",
      "Old data analysis - 0 To be extracted -  212\n",
      "Old data analysis - 1 To be extracted -  212\n",
      "{0: 212, 1: 212}\n",
      "IRB\n",
      "(423, 2) 424.0\n",
      "Old data analysis - 0 To be extracted -  106\n",
      "Old data analysis - 1 To be extracted -  106\n",
      "{0: 106, 1: 106}\n",
      "IRB\n",
      "(211, 2) 212.0\n",
      "old               precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8870    0.9128    0.8997       585\n",
      "           1     0.8886    0.8568    0.8725       475\n",
      "\n",
      "    accuracy                         0.8877      1060\n",
      "   macro avg     0.8878    0.8848    0.8861      1060\n",
      "weighted avg     0.8878    0.8877    0.8875      1060\n",
      "\n",
      "dumb               precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8908    0.8923    0.8915       585\n",
      "           1     0.8671    0.8653    0.8662       475\n",
      "\n",
      "    accuracy                         0.8802      1060\n",
      "   macro avg     0.8789    0.8788    0.8789      1060\n",
      "weighted avg     0.8802    0.8802    0.8802      1060\n",
      "\n",
      "Old data analysis - 0 To be extracted -  1060\n",
      "Old data analysis - 1 To be extracted -  1060\n",
      "{0: 1060, 1: 1060}\n",
      "IRB\n",
      "(2119, 2) 2120.0\n",
      "Old data analysis - 0 To be extracted -  848\n",
      "Old data analysis - 1 To be extracted -  848\n",
      "{0: 848, 1: 848}\n",
      "IRB\n",
      "(1695, 2) 1696.0\n",
      "Old data analysis - 0 To be extracted -  636\n",
      "Old data analysis - 1 To be extracted -  636\n",
      "{0: 636, 1: 636}\n",
      "IRB\n",
      "(1271, 2) 1272.0\n",
      "Old data analysis - 0 To be extracted -  424\n",
      "Old data analysis - 1 To be extracted -  424\n",
      "{0: 424, 1: 424}\n",
      "IRB\n",
      "(847, 2) 848.0\n",
      "Old data analysis - 0 To be extracted -  212\n",
      "Old data analysis - 1 To be extracted -  212\n",
      "{0: 212, 1: 212}\n",
      "IRB\n"
     ]
    }
   ],
   "source": [
    "framework_result=pd.DataFrame()\n",
    "use_case=\"REFDAT\"\n",
    "datasets=[(\"banana\",(1,-1)),\n",
    "        #(\"breast-cancer\",(\"recurrence-events\",\"no-recurrence-events\")),\n",
    "        (\"bupa\",(2,1)),\n",
    "        # (\"EEG\",(1,0)),\n",
    "        (\"magic\",(\"g\",\"h\")),\n",
    "        (\"monk-2\",(1,0)),\n",
    "        (\"pima\",(\"tested_positive\",\"tested_negative\")),\n",
    "        (\"ring\",(1,0)),\n",
    "        (\"twonorm\",(1,0)),\n",
    "        (\"phoneme\",(1,0))\n",
    "        ]\n",
    "with Parallel(n_jobs=1,backend=\"multiprocessing\") as parallel:\n",
    "    for data in datasets:\n",
    "        use_case=data[0]\n",
    "        old_data,test_data,indexs=prep_data(use_case=use_case,data=data,label=\"Class\")\n",
    "        original_data=old_data.csv_data.copy()\n",
    "        if old_data.csv_data.shape[0] < 500:\n",
    "            dumb_ratio=0.6\n",
    "        else: \n",
    "            dumb_ratio=0.8\n",
    "        fold=len(indexs)\n",
    "        for index in indexs:\n",
    "            old_data.csv_data=original_data.filter(items=index[0],axis=0)\n",
    "            test_data.csv_data=original_data.filter(items=index[1],axis=0)\n",
    "            # old_data.save(f\"{output_directory}{fold}_old_{use_case}_0.0.csv\")\n",
    "            # test_data.save(f\"{output_directory}{fold}_test_{use_case}.csv\")    \n",
    "            for i in range(3,4):\n",
    "                model_type=\"knn\"\n",
    "                clfs,res=prep_models(old_data,test_data,use_case,dumb_ratio=dumb_ratio,knn=i,model_type=model_type)\n",
    "                #framework_result=pd.concat([framework_result,res])\n",
    "                res=imbl_comp(old_data,test_data,clfs,use_case,i,fold)\n",
    "                \n",
    "                framework_result=pd.concat([framework_result,res])\n",
    "               \n",
    "                # Choose modification and K estimation method \n",
    "                for alg in []:#kmeans\n",
    "                    print(alg)\n",
    "                    if alg== 'h2o':\n",
    "                        k_methods=['default']\n",
    "                        h2o.init()\n",
    "                    else:\n",
    "                        k_methods=[\"bouldin\"]#,'calinski_harabasz','silhouette','elbow','bouldin','PRE']:\n",
    "                    for metric in k_methods:\n",
    "                        print(metric)\n",
    "                        for red in[0.1,0.05]:# [0.5,0.4,0.3,0.2,0.1,0.05]:#,0.04,0.03,0.02,0.01]:#[0.04,0.03,0.02,0.01]:#[0.8,0.7,0.6,0.5,0.4]:#0.3,0.2,0.1,0.05\n",
    "                            #try: \n",
    "                            input_vector,jobs,reduction=get_params(red,old_data=old_data,clfs=clfs)\n",
    "                            \n",
    "                            start_time = time.time()\n",
    "                            results=parallel([delayed(modification_one)(label=i[0],data=i[1],clf=i[2],undersample=i[3],metric=metric,alg=alg,desc=i[4]) for i in input_vector])\n",
    "                            end_time = time.time()\n",
    "                            elapsed_time = end_time - start_time\n",
    "                            df=pd.DataFrame({'time':elapsed_time,\"method\":\"Bouldin\",\"fold\":fold,\"dataset\":use_case,\"reduction\":reduction},index=[0])\n",
    "                            framework_result=pd.concat([framework_result,df])\n",
    "                            #old_data.newdata=pd.concat(results)\n",
    "                            ## random sampled res\n",
    "                            # fin_red=red/5\n",
    "                            # backup=old_data.csv_data\n",
    "                            # old_data.update_csv()\n",
    "                            # vec,_,reduction=get_params(0.17,old_data=old_data,clfs=clfs)\n",
    "                            # fin_balance_dict={0:vec[0][3],1:vec[1][3]}\n",
    "                            \n",
    "                            # old_data.apply_imbalanced_lib(RandomUnderSampler(sampling_strategy=fin_balance_dict))\n",
    "                            # old_data.set_data(backup)\n",
    "                            #print(old_data.newdata)\n",
    "                            #vizualize(old_data.newdata,test_data.csv_data)\n",
    "                            #old_data.save_new(f\"{output_directory}{fold}_bouldin_3NN_{use_case}_{reduction}.csv\",append=False)\n",
    "                            #print(test_data.csv_data)\n",
    "                            \n",
    "\n",
    "                            # clustered_result=old_data.compare_models(clfs['old'],clfs['clustered'],test_data)\n",
    "                            # tmp=old_d ata.prepare_model_result(clustered_result,text+\" \"+str(reduction),use_case,alg+'_'+metric+\" \"+str(i))\n",
    "                            # framework_result=pd.concat([framework_result,tmp])\n",
    "                            # except:\n",
    "                            #     pass\n",
    "                            #old_data.save_new(output_directory+use_case+\"_new_data_\"+alg+\"_\"+metric+\"_\"+str(reduction)+\".csv\")\n",
    "                        #test_data.save(f\"xsetin00/Data/reduced_datasets/test_{use_case}.csv\")     \n",
    "                    if alg == 'h2o':\n",
    "                        h2o.cluster().shutdown()\n",
    "                fold=fold-1\n",
    "\n",
    "framework_result.to_csv(f\"{output_directory}ser_times.csv\",index=False,mode=\"a\",header=False)\n",
    "                \n",
    "        "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Show results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print results\n",
    "framework_result#['Acc'].mean()\n",
    "framework_result.to_csv(f\"{output_directory}ser_times.csv\",index=False,mode=\"a\",header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "framework_result.to_csv(f\"{output_directory}inc_scaled_res.csv\",index=False,mode=\"a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OLD 0.6318840579710144\n",
      "Clustered nan\n",
      "Clustered 0.6000000000000001\n"
     ]
    }
   ],
   "source": [
    "print(\"OLD\",framework_result[framework_result[\"Metric\"]=='old'][\"Acc\"].mean())\n",
    "print(\"Clustered\",framework_result[framework_result[\"Metric\"]=='kmeans_elbow'][\"Acc\"].mean())\n",
    "print(\"Clustered\",framework_result[framework_result[\"Metric\"]=='kmeans_bouldin'][\"Acc\"].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OLD 0.8666037735849056\n",
      "Clustered 0.8615094339622642\n",
      "Clustered nan\n"
     ]
    }
   ],
   "source": [
    "print(\"OLD\",framework_result[framework_result[\"Metric\"]=='old'][\"Acc\"].mean())\n",
    "print(\"Clustered\",framework_result[framework_result[\"Metric\"]=='kmeans_elbow'][\"Acc\"].mean())\n",
    "print(\"Clustered\",framework_result[framework_result[\"Metric\"]=='kmeans_bouldin'][\"Acc\"].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results\n",
    "framework_result.to_csv(f\"{output_directory}{use_case}_{text}_result.csv\",mode='a',index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imbl_comp(old_data,test_data,clfs,use_case,knn,fold):\n",
    "    result=pd.DataFrame()\n",
    "    # Comaration with imbalanced learn library which implements various methods of data modification\n",
    "    for reduction in [0.5,0.4,0.3,0.2,0.1,0.05]:#,0.02,0.01]:#,0.04,0.03,0.02,0.01]:#,0.7,0.6,0.5,0.4]:#[1,0.8,0.7,0.6,0.5,0.4,0.3,0.2,0.1,0.05]:\n",
    "        vec,_,red=get_params(reduction,old_data=old_data,clfs=clfs)\n",
    "        \n",
    "        balance_dict={0:vec[0][3],1:vec[1][3]}\n",
    "        print(balance_dict)\n",
    "        \n",
    "        imbl=[(None,\"IRB\"),\n",
    "                ####(NearMiss(sampling_strategy=balance_dict,version=1),\"NearMiss-1\"),###########\n",
    "                ####(NearMiss(sampling_strategy=balance_dict,version=2),\"NearMiss-2\"),\n",
    "                ####(NearMiss(sampling_strategy=balance_dict,version=3),\"NearMiss-3\"),\n",
    "                # (NearMiss(sampling_strategy=balance_dict,n_neighbors=4,version=1,n_jobs=-1),\"4nn-nearmiss1\"),###########\n",
    "                # (NearMiss(sampling_strategy=balance_dict,n_neighbors=4,version=2,n_jobs=-1),\"4nn-nearmiss2\"),############\n",
    "                # (NearMiss(sampling_strategy=balance_dict,n_neighbors=4,version=3,n_jobs=-1),\"4nn-3nearmiss3\"),\n",
    "                # (NearMiss(sampling_strategy=balance_dict,n_neighbors=5,version=1,n_jobs=-1),\"4nn-nearmiss1\"),###########\n",
    "                # (NearMiss(sampling_strategy=balance_dict,n_neighbors=5,version=2,n_jobs=-1),\"4nn-nearmiss2\"),############\n",
    "                # (NearMiss(sampling_strategy=balance_dict,n_neighbors=5,version=3,n_jobs=-1),\"4nn-3nearmiss3\"),\n",
    "                # (NearMiss(sampling_strategy=balance_dict,n_neighbors=3,n_neighbors_ver3=2,version=3,n_jobs=-1),\"3nn-2nearmiss3\"),###########\n",
    "                # (NearMiss(sampling_strategy=balance_dict,n_neighbors=4,n_neighbors_ver3=2,version=3,n_jobs=-1),\"4nn-2nearmiss3\"),############\n",
    "                # (NearMiss(sampling_strategy=balance_dict,n_neighbors=5,n_neighbors_ver3=2,version=3,n_jobs=-1),\"5nn-2nearmiss3\"),\n",
    "                # (NearMiss(sampling_strategy=balance_dict,n_neighbors=3,n_neighbors_ver3=4,version=3,n_jobs=-1),\"3nn-4nearmiss3\"),###########\n",
    "                # (NearMiss(sampling_strategy=balance_dict,n_neighbors=4,n_neighbors_ver3=4,version=3,n_jobs=-1),\"4nn-4nearmiss3\"),############\n",
    "                # (NearMiss(sampling_strategy=balance_dict,n_neighbors=5,n_neighbors_ver3=4,version=3,n_jobs=-1),\"5nn-4nearmiss3\"),\n",
    "                # (NearMiss(sampling_strategy=balance_dict,n_neighbors=3,n_neighbors_ver3=5,version=3,n_jobs=-1),\"3nn-5nearmiss3\"),###########\n",
    "                # (NearMiss(sampling_strategy=balance_dict,n_neighbors=4,n_neighbors_ver3=5,version=3,n_jobs=-1),\"4nn-5nearmiss3\"),############\n",
    "                # (NearMiss(sampling_strategy=balance_dict,n_neighbors=5,n_neighbors_ver3=5,version=3,n_jobs=-1),\"5nn-5nearmiss3\"),\n",
    "                #(CondensedNearestNeighbour(random_state=42,sampling_strategy='all',n_jobs=-1),\"condensed_NN\"),\n",
    "                #(EditedNearestNeighbours(),\"edited_NN\"),\n",
    "                #(RepeatedEditedNearestNeighbours(sampling_strategy='all',n_jobs=-1),\"rep_edited__NN\"),\n",
    "                #(InstanceHardnessThreshold(sampling_strategy=balance_dict,random_state=42, estimator=LogisticRegression(solver='lbfgs', max_iter=200)),\"inst_hard_tresh\"),########\n",
    "                #(NeighbourhoodCleaningRule(sampling_strategy='all',n_jobs=-1),\"neigh_clean_rule\"),\n",
    "                #(OneSidedSelection(sampling_strategy='all',n_jobs=-1,random_state=42),\"one_sided_sel\"),\n",
    "                ####(RandomUnderSampler(sampling_strategy=balance_dict),\"RandomUnderSampler\"),#ranmagic random_state=1\n",
    "                #(TomekLinks(sampling_strategy='all',n_jobs=-1),\"tomek_links\"),\n",
    "                #(AllKNN(sampling_strategy='all',n_jobs=-1,n_neighbors=7),\"allKNN7\"),\n",
    "                ####(ClusterCentroids(sampling_strategy=balance_dict),\"ClusterCentroids\"),#########\n",
    "                #(ClusterCentroids(sampling_strategy=balance_dict,random_state=42,estimator=KMeans(n_clusters=cluster_model.size)),\"cluster_centroids_est_k\")\n",
    "                ]\n",
    "        \n",
    "        #final_red=reduction/5\n",
    "        \n",
    "        mixed_data=pd.DataFrame()\n",
    "        for sampler in imbl:\n",
    "            print(sampler[1])\n",
    "            start_time = time.time()\n",
    "            #old_data.apply_imbalanced_lib(sampler[0])\n",
    "            ### IRB\n",
    "            X=old_data.get_all_features()\n",
    "            y=old_data.get_all_label()\n",
    "            X,y=apply_irb(X.to_numpy(),y.to_numpy(),red)\n",
    "            X=pd.DataFrame(X,columns=old_data.get_all_features().columns)\n",
    "            y=pd.DataFrame(y,columns=[old_data.get_all_label().name])\n",
    "            old_data.set_newdata(X,y)\n",
    "            ###\n",
    "            end_time = time.time()\n",
    "            elapsed_time = end_time - start_time   \n",
    "            df=pd.DataFrame({'time':elapsed_time,\"method\":sampler[1],\"fold\":fold,\"dataset\":use_case,\"reduction\":red},index=[0])\n",
    "            result=pd.concat([result,df])\n",
    "            ## random sampled union\n",
    "            # backup=old_data.csv_data\n",
    "            # old_data.update_csv()\n",
    "            # vec,_,red=get_params(0.17,old_data=old_data,clfs=clfs)\n",
    "            # fin_balance_dict={0:vec[0][3],1:vec[1][3]}\n",
    "            \n",
    "            # old_data.apply_imbalanced_lib(RandomUnderSampler(sampling_strategy=fin_balance_dict))\n",
    "            # kdtree = cKDTree(backup.values)\n",
    "            # _, idx = kdtree.query(old_data.newdata.values, k=1)\n",
    "            # old_data.newdata=backup.iloc[idx.flatten()]\n",
    "            # old_data.set_data(backup)\n",
    "            ##\n",
    "            #mixed_data=pd.concat([mixed_data,old_data.newdata])\n",
    "            old_data.save_new(f\"{output_directory}{fold}_{sampler[1]}_{use_case}_{red}.csv\")\n",
    "            #vizualize(old_data.newdata,test_data.csv_data)\n",
    "            #imbl_result=old_data.compare_models(clfs['old'],clfs['compare'],test_data,sampler[1])\n",
    "            #reduce=1-old_data.cnt_new['all']/old_data.cnt_old['all']\n",
    "            #tmp=old_data.prepare_model_result(imbl_result,sampler[1]+\" \"+str(red),use_case,sampler[1]+\" \"+str(knn))\n",
    "            #result=pd.concat([result,tmp])\n",
    "        # old_data.newdata=mixed_data\n",
    "        # old_data.save_new(f\"{output_directory}{fold}_grouped_{use_case}_{reduction}.csv\")\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IRB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 5) 10.0\n"
     ]
    }
   ],
   "source": [
    "from operator import itemgetter\n",
    "def applyENNFilter(T):\n",
    "    # Implement your filtering logic here\n",
    "    # For example, using k-Nearest Neighbors to filter instances\n",
    "    Tenn = T  # Replace with your filtering code\n",
    "    return Tenn\n",
    "\n",
    "def calculate_similarity(pi, pj):\n",
    "    # use euclidean distance q\n",
    "    return np.linalg.norm(pi - pj)\n",
    "\n",
    "def rank_instance(pi, model, k):\n",
    "    rank = 0\n",
    "    sim_score=0\n",
    "    \n",
    "    distances, _ = model.kneighbors(pi.reshape(1, -1))\n",
    "    # print(distances,indices)\n",
    "    for i in range(k):\n",
    "        sim_score+=distances[0][i]   \n",
    "    return sim_score / k\n",
    "\n",
    "def selectInstancesFrom(C, high_percent, medium_percent, low_percent):\n",
    "    n = len(C)\n",
    "    high_count = int(high_percent * n)\n",
    "    medium_count = int(medium_percent * n)\n",
    "    low_count = int(low_percent * n)\n",
    "    \n",
    "    high_ranked = C[:high_count]\n",
    "    medium_ranked = C[high_count:high_count + medium_count]\n",
    "    low_ranked = C[high_count + medium_count: high_count + medium_count + low_count]\n",
    "    \n",
    "    return high_ranked, medium_ranked, low_ranked\n",
    "\n",
    "def selectInstances(T, high, medium, low, k):\n",
    "    Tenn = applyENNFilter(T)\n",
    "    \n",
    "    ranks={0:[],1:[]}\n",
    "    for class_label, class_instances in Tenn.items():\n",
    "        model = NearestNeighbors(n_neighbors=k, algorithm='auto')\n",
    "        model.fit(Tenn[1-class_label])\n",
    "        for instance in class_instances:\n",
    "            # print(class_label)\n",
    "            # print(instance)\n",
    "            instance_rank = rank_instance(instance, model, k)\n",
    "            ranks[class_label].append(instance_rank)\n",
    "    \n",
    "    Tout = {}\n",
    "    for class_label, class_instances in Tenn.items():\n",
    "        # print(class_instances)\n",
    "        combined=list(zip(class_instances,ranks[class_label]))\n",
    "        \n",
    "        combined.sort(key=itemgetter(1))\n",
    "        #print(combined)\n",
    "        sorted_by_rank=[x for x,_ in combined]\n",
    "        # print(sorted_by_rank)\n",
    "        high_ranked, medium_ranked, low_ranked = selectInstancesFrom(sorted_by_rank, high, medium, low)\n",
    "        # print(len(high_ranked),len(medium_ranked),len(low_ranked))\n",
    "        selected_instances = []\n",
    "        selected_instances.extend(high_ranked + medium_ranked + low_ranked)\n",
    "        Tout[class_label] = selected_instances  \n",
    "  \n",
    "    # print(len(Tout[0]),len(Tout[1]))\n",
    "    X=np.concatenate((Tout[0],Tout[1]))\n",
    "    y=np.concatenate((np.zeros(len(Tout[0]),dtype=int),np.ones(len(Tout[1]),dtype=int)))\n",
    "    return X,y\n",
    "\n",
    "def apply_irb(X,y,ratio):\n",
    "   \n",
    "    # X=X.to_numpy()\n",
    "    # y=y.to_numpy()\n",
    "    #print(X,y)\n",
    "    size=X.shape[0]\n",
    "    T = {0: X[y == 0], 1: X[y == 1]}\n",
    "    high_percentage = ratio\n",
    "    medium_percentage = 0.0\n",
    "    low_percentage = 0.0\n",
    "    k_neighbors = 3\n",
    "\n",
    "    X,y = selectInstances(T, high_percentage, medium_percentage, low_percentage, k_neighbors)\n",
    "    # print(X,y)\n",
    "    print(X.shape,size*(high_percentage+medium_percentage+low_percentage))\n",
    "    return X,y\n",
    "\n",
    "# Example usage:\n",
    "# Define your dataset T and other parameters\n",
    "# define numpy array T with two classes and 10 instances each 5 features\n",
    "size=100\n",
    "X1 = np.random.rand(size, 5)\n",
    "X2 = np.random.rand(size, 5)\n",
    "y1 = np.ones(size)\n",
    "y2 = np.zeros(size)\n",
    "X=np.concatenate((X1,X2))\n",
    "y=np.concatenate((y1,y2))\n",
    "\n",
    "ratio=0.05\n",
    "X,y=apply_irb(X,y,ratio)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "wPe_TlSVBXGI",
    "Y3zOkaJfndzC",
    "E_RJaD0si8lO",
    "ZeFW6--apPb4",
    "m4wVGJPImtz9",
    "LzkWohqEpEjZ"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
